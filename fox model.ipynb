{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pakacges\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/fox project/train/', '/fox project/test/')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = \"/fox project/train/\"\n",
    "test = \"/fox project/test/\"\n",
    "\n",
    "train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 directories and 1 images in '/fox project/'.\n",
      "There are 2 directories and 0 images in '/fox project/test'.\n",
      "There are 0 directories and 445 images in '/fox project/test\\fox'.\n",
      "There are 0 directories and 120 images in '/fox project/test\\no_fox'.\n",
      "There are 2 directories and 0 images in '/fox project/train'.\n",
      "There are 0 directories and 1568 images in '/fox project/train\\fox'.\n",
      "There are 0 directories and 1333 images in '/fox project/train\\no_fox'.\n"
     ]
    }
   ],
   "source": [
    "for dirpath, dirnames, filenames in os.walk(\"/fox project/\"):\n",
    "    print(f\"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#image sizes\n",
    "\n",
    "image_hight = 128\n",
    "image_wildth = 128\n",
    "\n",
    "image_size = (image_hight,image_wildth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#switching to sensor + resizing\n",
    "image_transform = transforms.Compose([\n",
    "transforms.Resize(size=image_size),\n",
    "transforms.RandomHorizontalFlip(p=0.5),\n",
    "transforms.ToTensor()\n",
    "]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transform = transforms.Compose([\n",
    "transforms.Resize(size=image_size),\n",
    "transforms.ToTensor()\n",
    "]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = datasets.ImageFolder(train,image_transform,target_transform=None)\n",
    "test_data = datasets.ImageFolder(test,test_transform,target_transform=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fox', 'no_fox']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#classes investigation\n",
    "print(train_data.classes)\n",
    "len(train_data.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "workers = os.cpu_count()\n",
    "\n",
    "train_load = DataLoader(dataset=train_data,batch_size=32,num_workers=workers,shuffle=True)\n",
    "test_load = DataLoader(dataset=test_data,batch_size=32,num_workers=workers,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2_load = DataLoader(dataset=train_data,batch_size=32,num_workers=workers,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a CNN-based image classifier.\n",
    "class ImageClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv_layer_1 = nn.Sequential(\n",
    "          nn.Conv2d(3, 64, 3, padding=1),\n",
    "          nn.ReLU(),\n",
    "          nn.BatchNorm2d(64),\n",
    "          nn.MaxPool2d(2))\n",
    "        self.conv_layer_2 = nn.Sequential(\n",
    "          nn.Conv2d(64, 512, 3, padding=1),\n",
    "          nn.ReLU(),\n",
    "          nn.BatchNorm2d(512),\n",
    "          nn.MaxPool2d(2))\n",
    "        self.conv_layer_3 = nn.Sequential(\n",
    "          nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "          nn.ReLU(),\n",
    "          nn.BatchNorm2d(512),\n",
    "          nn.MaxPool2d(2)) \n",
    "        self.classifier = nn.Sequential(\n",
    "          nn.Flatten(),\n",
    "          nn.Linear(in_features=2048, out_features=2))\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.conv_layer_1(x)\n",
    "        x = self.conv_layer_2(x)\n",
    "        x = self.conv_layer_3(x)\n",
    "        x = self.conv_layer_3(x)\n",
    "        x = self.conv_layer_3(x)\n",
    "        x = self.conv_layer_3(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "# Instantiate an object.\n",
    "model = ImageClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_batch, label_batch = next(iter(train_load))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_single, label_single = img_batch[0].unsqueeze(dim=0), label_batch[0]\n",
    "label_single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5043, 0.4957]])\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.inference_mode():\n",
    "    pred = model(img_single)\n",
    "\n",
    "print(torch.softmax(pred, dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    10] loss: 0.076\n",
      "[1,    20] loss: 0.076\n",
      "[1,    30] loss: 0.076\n",
      "[1,    40] loss: 0.076\n",
      "[1,    50] loss: 0.076\n",
      "[1,    60] loss: 0.076\n",
      "[1,    70] loss: 0.076\n",
      "[1,    80] loss: 0.076\n",
      "[1,    90] loss: 0.076\n",
      "[2,    10] loss: 0.076\n",
      "[2,    20] loss: 0.076\n",
      "[2,    30] loss: 0.076\n",
      "[2,    40] loss: 0.076\n",
      "[2,    50] loss: 0.075\n",
      "[2,    60] loss: 0.076\n",
      "[2,    70] loss: 0.075\n",
      "[2,    80] loss: 0.075\n",
      "[2,    90] loss: 0.076\n",
      "[3,    10] loss: 0.076\n",
      "[3,    20] loss: 0.076\n",
      "[3,    30] loss: 0.076\n",
      "[3,    40] loss: 0.076\n",
      "[3,    50] loss: 0.075\n",
      "[3,    60] loss: 0.075\n",
      "[3,    70] loss: 0.076\n",
      "[3,    80] loss: 0.075\n",
      "[3,    90] loss: 0.076\n",
      "[4,    10] loss: 0.076\n",
      "[4,    20] loss: 0.076\n",
      "[4,    30] loss: 0.076\n",
      "[4,    40] loss: 0.075\n",
      "[4,    50] loss: 0.075\n",
      "[4,    60] loss: 0.076\n",
      "[4,    70] loss: 0.075\n",
      "[4,    80] loss: 0.076\n",
      "[4,    90] loss: 0.075\n",
      "[5,    10] loss: 0.076\n",
      "[5,    20] loss: 0.075\n",
      "[5,    30] loss: 0.075\n",
      "[5,    40] loss: 0.075\n",
      "[5,    50] loss: 0.075\n",
      "[5,    60] loss: 0.075\n",
      "[5,    70] loss: 0.075\n",
      "[5,    80] loss: 0.075\n",
      "[5,    90] loss: 0.075\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(5):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_load, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 10 == 9:    # print every 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / len(train_load):.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 78 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data in test_load:\n",
    "        images, labels = data\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = model(images)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_path = \"D:/fox project/customfox.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_res_pic = Image.open(custom_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_res_test = test_transform(low_res_pic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom image transformed shape: torch.Size([3, 128, 128])\n",
      "Unsqueezed custom image shape: torch.Size([1, 3, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "with torch.inference_mode():\n",
    "    # Add an extra dimension to image\n",
    "    custom_image_transformed_with_batch_size = low_res_test.unsqueeze(dim=0)\n",
    "    \n",
    "    # Print out different shapes\n",
    "    print(f\"Custom image transformed shape: {low_res_test.shape}\")\n",
    "    print(f\"Unsqueezed custom image shape: {custom_image_transformed_with_batch_size.shape}\")\n",
    "    \n",
    "    # Make a prediction on image with an extra dimension\n",
    "    custom_image_pred = model(low_res_test.unsqueeze(dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction logits: tensor([[ 0.0387, -0.0628]])\n",
      "Prediction probabilities: tensor([[0.5254, 0.4746]])\n",
      "Prediction label: tensor([0])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Let's convert them from logits -> prediction probabilities -> prediction labels\n",
    "# Print out prediction logits\n",
    "print(f\"Prediction logits: {custom_image_pred}\")\n",
    "\n",
    "# Convert logits -> prediction probabilities (using torch.softmax() for multi-class classification)\n",
    "custom_image_pred_probs = torch.softmax(custom_image_pred, dim=1)\n",
    "print(f\"Prediction probabilities: {custom_image_pred_probs}\")\n",
    "\n",
    "# Convert prediction probabilities -> prediction labels\n",
    "custom_image_pred_label = torch.argmax(custom_image_pred_probs, dim=1)\n",
    "print(f\"Prediction label: {custom_image_pred_label}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
